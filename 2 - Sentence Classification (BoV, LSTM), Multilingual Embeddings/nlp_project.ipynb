{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project\n",
    "\n",
    "### Student : Yonatan DELORO\n",
    "\n",
    "### Contact : yonatan.deloro@eleves.enpc.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonatan/.local/lib/python2.7/site-packages/h5py-2.7.1-py2.7-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Dropout, Flatten, Reshape\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        #self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.word2id = {word: idx for (idx, word) in enumerate(list(self.word2vec.keys()))}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "        \n",
    "   \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ',1)\n",
    "                #punctuations might be useful for sentiment analysis\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')                \n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "                        \n",
    "        idx_w = self.word2id[w]\n",
    "        embedding_w = self.embeddings[idx_w]\n",
    "        \n",
    "        knn = self.most_similar_to_embedding(embedding_w, K=K+1)\n",
    "        #we ask for the K+1 most similar words\n",
    "        #as the word w belong itself to word2vec and will be the first output in knn\n",
    "        \n",
    "        return knn[1:]\n",
    "\n",
    "    def most_similar_to_embedding(self,embedding, K=5):\n",
    "        # K most similar words to the given embedding\n",
    "        # intermediary function built for part II (multilingual embedding)\n",
    "        \n",
    "        N_words = len(self.id2word)\n",
    "        \n",
    "        scores = np.zeros(N_words)\n",
    "        for idx in self.id2word.keys():\n",
    "            scores[idx] = self.similarity(embedding,self.embeddings[idx])\n",
    "        \n",
    "        idx_sorted = np.argsort(-scores) #index of words sorted by decreasing similarity\n",
    "        idx_knn = idx_sorted[:K].flatten() #idx of K most similar words\n",
    "                \n",
    "        words = []\n",
    "        for i in range(idx_knn.shape[0]):\n",
    "            word = self.id2word[idx_knn[i]]\n",
    "            words.append(word)\n",
    "            \n",
    "        return words\n",
    "        \n",
    "    def similarity(self, embedding1, embedding2):\n",
    "        \n",
    "        norm1 = np.linalg.norm(embedding1)\n",
    "        norm2 = np.linalg.norm(embedding2)\n",
    "        \n",
    "        return np.dot(embedding1,embedding2)/(norm1*norm2)\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        \n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        embedding_w1 = self.embeddings[self.word2id[w1]]\n",
    "        embedding_w2 = self.embeddings[self.word2id[w2]]\n",
    "        \n",
    "        return self.similarity(embedding_w1, embedding_w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "similarity scores : \n",
      "('cat', 'dog', 0.671683666279249)\n",
      "('dog', 'pet', 0.6842064029669219)\n",
      "('dogs', 'cats', 0.7074389328052405)\n",
      "('paris', 'france', 0.7775108541288561)\n",
      "('germany', 'berlin', 0.7420295235998394)\n",
      "##########\n",
      "\n",
      "cat\n",
      "most similar words :\n",
      "cats\n",
      "kitty\n",
      "kitten\n",
      "feline\n",
      "kitties\n",
      "\n",
      "dog\n",
      "most similar words :\n",
      "dogs\n",
      "puppy\n",
      "Dog\n",
      "doggie\n",
      "canine\n",
      "\n",
      "dogs\n",
      "most similar words :\n",
      "dog\n",
      "pooches\n",
      "Dogs\n",
      "doggies\n",
      "canines\n",
      "\n",
      "paris\n",
      "most similar words :\n",
      "france\n",
      "Paris\n",
      "parisian\n",
      "london\n",
      "berlin\n",
      "\n",
      "germany\n",
      "most similar words :\n",
      "austria\n",
      "europe\n",
      "german\n",
      "berlin\n",
      "poland\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(\"similarity scores : \")\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "\n",
    "print(\"##########\")\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(\"\\n\"+w1)\n",
    "    print(\"most similar words :\")\n",
    "    for w in w2v.most_similar(w1):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "\n",
    "        for sent in sentences:\n",
    "                        \n",
    "            embedding = np.zeros(300)\n",
    "            num_words = 0\n",
    "            words = sent.split(' ')\n",
    "                        \n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                for word in words:\n",
    "                    try: #to cover the case where word is not in self.w2v.word2id.keys():\n",
    "                        embedding += self.w2v.embeddings[self.w2v.word2id[word]]        \n",
    "                        num_words += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                for word in words:\n",
    "                    try: #to cover the case where word is not in self.w2v.word2id.keys():\n",
    "                        id_word = self.w2v.word2id[word]\n",
    "                        embedding += self.w2v.embeddings[id_word]*idf[id_word]\n",
    "                        num_words += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if num_words>0:\n",
    "                embedding /= num_words\n",
    "            sentemb.append(embedding)\n",
    "                \n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf=idf)\n",
    "        query = self.encode([s], idf=idf)\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(keys.shape[0]):\n",
    "            scores.append(self.similarity(query.flatten(), keys[i].flatten()))\n",
    "            \n",
    "        scores = np.array(scores)        \n",
    "        idx_sorted = np.argsort(-scores) #index of sentences sorted by decreasing similarity\n",
    "        idx_knn = idx_sorted[:K+1].flatten() #idx of K+1 most similar sentences\n",
    "                                             #(includes the sentence itself)\n",
    "        print(\"\\nconsidered sentence : \\n\")\n",
    "        print(s)\n",
    "        print(\"most similar sentences : \")\n",
    "        \n",
    "        res = []\n",
    "        for i in idx_knn[1:]:\n",
    "            sent = sentences[i]\n",
    "            print(\"\\n\"+sent)\n",
    "            res.append(sent)\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def similarity(self, embedding1, embedding2):\n",
    "        return np.dot(embedding1,embedding2)/(np.linalg.norm(embedding1)*np.linalg.norm(embedding2))\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        embedding1 = self.encode([s1],idf=idf).flatten()\n",
    "        embedding2 = self.encode([s2],idf=idf).flatten()\n",
    "        return self.similarity(embedding1, embedding2)\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "\n",
    "        num_sentences = 0\n",
    "        for sent in sentences:\n",
    "            num_sentences +=1\n",
    "            words = sent.split(' ')\n",
    "            for word in set(words): #set(words) to count only 1 if the word appears multiple times in the document\n",
    "                try: #to cover the case where word is not in self.w2v.word2id.keys():\n",
    "                    idx = self.w2v.word2id[word]\n",
    "                    if idx in idf.keys():\n",
    "                        idf[idx] += 1\n",
    "                    else:\n",
    "                        idf[idx] = 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        for (idx,value) in idf.items():\n",
    "            idf[idx] = np.log(num_sentences / value)\n",
    "            \n",
    "        return idf\n",
    "    \n",
    "def load_sentences(fname):\n",
    "    sentences = []\n",
    "    with io.open(PATH_TO_DATA+fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############\n",
      "BoV-mean\n",
      "\n",
      "considered sentence : \n",
      "\n",
      "1 woman in a black jacket is drinking out of a bottle while others are smiling . \n",
      "\n",
      "most similar sentences : \n",
      "\n",
      "a man in a pink shirt and a woman in a teal shirt , who is drinking wine , are sitting in a dark corner and talking . \n",
      "\n",
      "\n",
      "a woman in a blue shirt is laughing with a woman in a black shirt outside , as people are sitting and standing all around them . \n",
      "\n",
      "\n",
      "a man in a black jacket with a white stripe on the sleeve is holding a drink and laughing while a woman watches . \n",
      "\n",
      "\n",
      "a man in a white shirt squirting a bottle of a drink and smiling . \n",
      "\n",
      "\n",
      "the women in a red jacket is drinking from her cup , while the man in green cap is staring to his left . \n",
      "\n",
      "##############\n",
      "sentence 1 : 1 man standing and several people sitting down waiting on a subway train . \n",
      "\n",
      "sentence 2 : 10 women dressed in long black dresses holding a booklet up sheet music in front of them singing in a choir. you can see the back of 3 older gentlemen heads who appear to be the audience \n",
      "\n",
      "('similarity score : ', 0.7894217938961856)\n",
      "\n",
      "##############\n",
      "##############\n",
      "\n",
      "BoV-idf\n",
      "\n",
      "considered sentence : \n",
      "\n",
      "1 woman in a black jacket is drinking out of a bottle while others are smiling . \n",
      "\n",
      "most similar sentences : \n",
      "\n",
      "two women , one holding a beer bottle and smiling , the other holding a blue cup and looking down . \n",
      "\n",
      "\n",
      "a man wearing a yellow shirt is holding a bottle while walking with a crowd of people . \n",
      "\n",
      "\n",
      "a female drinking something out of a bottle outside . \n",
      "\n",
      "\n",
      "a man in a yellow shirt and jeans is walking down the street drinking a beverage from a bottle . \n",
      "\n",
      "\n",
      "the women in a red jacket is drinking from her cup , while the man in green cap is staring to his left . \n",
      "\n",
      "##############\n",
      "sentence 1 : 1 man standing and several people sitting down waiting on a subway train . \n",
      "\n",
      "sentence 2 : 10 women dressed in long black dresses holding a booklet up sheet music in front of them singing in a choir. you can see the back of 3 older gentlemen heads who appear to be the audience \n",
      "\n",
      "('similarity score : ', 0.6479653234348648)\n"
     ]
    }
   ],
   "source": [
    "#w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = load_sentences(\"sentences.txt\")\n",
    "        \n",
    "idf  = s2v.build_idf(sentences)\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "print(\"##############\")\n",
    "\n",
    "print(\"BoV-mean\")\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "\n",
    "print(\"##############\")\n",
    "\n",
    "sent1 = sentences[7]\n",
    "sent2 = sentences[13]\n",
    "print(\"sentence 1 : \"+sent1)\n",
    "print(\"sentence 2 : \"+sent2)\n",
    "print(\"similarity score : \",s2v.score('' if not sentences else sent1, '' if not sentences else sent2))\n",
    "\n",
    "print(\"\\n##############\")\n",
    "print(\"##############\\n\")\n",
    "print(\"BoV-idf\")\n",
    "\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "\n",
    "print(\"##############\")\n",
    "\n",
    "print(\"sentence 1 : \"+sent1)\n",
    "print(\"sentence 2 : \"+sent2)\n",
    "print(\"similarity score : \",s2v.score('' if not sentences else sent1, '' if not sentences else sent2, idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "#in terminal line from the data folder, I first wrote to download the data :\n",
    "#wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vecz\n",
    "\n",
    "N = 50000\n",
    "        \n",
    "w2v = {}\n",
    "w2v[\"fr\"] = Word2vec(PATH_TO_DATA+\"wiki.fr.vec\", N) \n",
    "w2v[\"en\"] = Word2vec(PATH_TO_DATA+\"wiki.en.vec\", N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((18970, 300), (18970, 300))\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "anchors = set(w2v[\"fr\"].word2id.keys()).intersection(set(w2v[\"en\"].word2id.keys()))\n",
    "anchors = list(anchors) #anchor words\n",
    "\n",
    "anchors_id = {\"fr\": [], \"en\": []}\n",
    "for word in anchors:\n",
    "    anchors_id[\"fr\"].append(w2v[\"fr\"].word2id[word])  #id of achor_word in french w2v\n",
    "    anchors_id[\"en\"].append(w2v[\"en\"].word2id[word])  #id of achor_word in english w2v\n",
    "    \n",
    "X = w2v[\"fr\"].embeddings[anchors_id[\"fr\"]]\n",
    "Y = w2v[\"en\"].embeddings[anchors_id[\"en\"]]\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "YTX = Y.T.dot(X)    \n",
    "U, s, VT = scipy.linalg.svd(YTX, full_matrices=True)\n",
    "W  = U.dot(VT)\n",
    "\n",
    "#English to French mapping\n",
    "Winv = np.linalg.inv(W)\n",
    "\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to French\n",
      "\n",
      "word : ovate\n",
      "French nearest neighbors : \n",
      "lancéolées\n",
      "pétiole\n",
      "bractées\n",
      "sépales\n",
      "folioles\n",
      "\n",
      "word : agonists\n",
      "French nearest neighbors : \n",
      "sérotonine\n",
      "inhibiteurs\n",
      "récepteurs\n",
      "inhibiteur\n",
      "inflammatoires\n",
      "\n",
      "word : église\n",
      "French nearest neighbors : \n",
      "église\n",
      "eglise\n",
      "paroissiale\n",
      "chapelle\n",
      "abbatiale\n",
      "\n",
      "word : stalker\n",
      "French nearest neighbors : \n",
      "scream\n",
      "psychopathe\n",
      "killer\n",
      "punisher\n",
      "sadique\n",
      "\n",
      "word : cheddar\n",
      "French nearest neighbors : \n",
      "cheese\n",
      "fromage\n",
      "worcestershire\n",
      "jambon\n",
      "oignons\n",
      "\n",
      "##########################\n",
      "\n",
      "French to English\n",
      "\n",
      "word : décision\n",
      "English nearest neighbors : \n",
      "decision\n",
      "decisions\n",
      "overrule\n",
      "sanction\n",
      "deliberations\n",
      "\n",
      "word : agglo\n",
      "English nearest neighbors : \n",
      "agglomeration\n",
      "essonne\n",
      "morbihan\n",
      "charentes\n",
      "sarthe\n",
      "\n",
      "word : dietrich\n",
      "English nearest neighbors : \n",
      "dietrich\n",
      "eberhard\n",
      "helmuth\n",
      "bernhard\n",
      "wilhelm\n",
      "\n",
      "word : delattre\n",
      "English nearest neighbors : \n",
      "marchand\n",
      "gauthier\n",
      "françois\n",
      "lacroix\n",
      "clément\n",
      "\n",
      "word : salat\n",
      "English nearest neighbors : \n",
      "prat\n",
      "salah\n",
      "garonne\n",
      "allah\n",
      "ariège\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "def foreign_neighbors(word, foreign=\"fr\", K=5):\n",
    "\n",
    "    if foreign==\"fr\":\n",
    "        embedding_word_en = w2v[\"en\"].embeddings[w2v[\"en\"].word2id[word]]\n",
    "        embedding_word_fr = Winv.dot(embedding_word_en)\n",
    "        return w2v[\"fr\"].most_similar_to_embedding(embedding_word_fr)\n",
    "    \n",
    "    elif foreign==\"en\":\n",
    "        embedding_word_fr = w2v[\"fr\"].embeddings[w2v[\"fr\"].word2id[word]]\n",
    "        embedding_word_en = W.dot(embedding_word_fr)\n",
    "        return w2v[\"en\"].most_similar_to_embedding(embedding_word_en)\n",
    "    \n",
    "    else:\n",
    "        print(\"'foreign' should be 'fr' or 'en'\")\n",
    "    \n",
    "    \n",
    "print(\"English to French\")\n",
    "words_en = np.random.choice(w2v[\"en\"].word2id.keys(),5)\n",
    "for word in words_en:\n",
    "    print(\"\\nword : \"+word)\n",
    "    print(\"French nearest neighbors : \")\n",
    "    for w in foreign_neighbors(word, foreign=\"fr\", K=5):\n",
    "        print(w)\n",
    "    #print([w.format() )\n",
    "    \n",
    "print(\"\\n##########################\\n\")\n",
    "\n",
    "print(\"French to English\")\n",
    "words_fr = np.random.choice(w2v[\"fr\"].word2id.keys(),5)\n",
    "for word in words_fr:\n",
    "    print(\"\\nword : \"+word)\n",
    "    print(\"English nearest neighbors : \")\n",
    "    for w in foreign_neighbors(word, foreign=\"en\", K=5):\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "def load_sentences_with_labels(fname):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with io.open(PATH_TO_DATA+fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            labels.append(int(line[0]))\n",
    "            sentences.append(line[2:])\n",
    "    return sentences, labels\n",
    "\n",
    "sentences = {}\n",
    "labels = {}\n",
    "sets = [\"train\",\"dev\",\"test.X\"]\n",
    "\n",
    "sentences[\"train\"], labels[\"train\"] = load_sentences_with_labels(\"SST/stsa.fine.train\")\n",
    "sentences[\"dev\"], labels[\"dev\"] = load_sentences_with_labels(\"SST/stsa.fine.dev\")\n",
    "sentences[\"test.X\"] = load_sentences(\"SST/stsa.fine.test.X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "#w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "all_sentences = sentences[\"train\"] + sentences[\"dev\"] + sentences[\"test.X\"]\n",
    "idf  = s2v.build_idf(all_sentences)\n",
    "\n",
    "embeddings = {}\n",
    "for s in sets:\n",
    "    #embeddings[s] = s2v.encode(sentences[s])\n",
    "    embeddings[s] = s2v.encode(sentences[s], idf = idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chosen alpha : ', 90.0)\n",
      "('score on train : ', 0.47992508486480157)\n",
      "('score on dev : ', 0.42727272727272725)\n",
      "('loss on train : ', 1.221294735034186)\n",
      "('loss on dev : ', 1.3178251269927477)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#looking for best best alpha (l2 regularization intensity)\n",
    "#set_alpha = [10**(p-5) for p in range(10)] #first, coarse\n",
    "set_alpha = np.linspace(50,150,21) #then, finer\n",
    "losses_dev = []\n",
    "\n",
    "for alpha in set_alpha:\n",
    "    \n",
    "    # from scikit documentation : uses the cross- entropy loss if the ‘multi_class’ option is set to ‘multinomial’\n",
    "    LR = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",\n",
    "                            penalty='l2', C=1./alpha)\n",
    "        \n",
    "    LR.fit(embeddings[\"train\"],labels[\"train\"])\n",
    "    #print(LR.score(embeddings[\"dev\"],labels[\"dev\"]))\n",
    "    predictions_probas = LR.predict_proba(embeddings[\"dev\"])\n",
    "    losses_dev.append(log_loss(labels[\"dev\"],predictions_probas))\n",
    "\n",
    "    \n",
    "    \n",
    "#plotting loss on dev against alpha\n",
    "#plt.semilogx(set_alpha,losses_dev)\n",
    "plt.plot(set_alpha,losses_dev)\n",
    "plt.xlabel(\"alpha (regularization cste)\")\n",
    "plt.ylabel(\"cross-entropy loss on dev set\")\n",
    "plt.show()\n",
    "\n",
    "idx_best_alpha = np.argmin(losses_dev) \n",
    "chosen_alpha = set_alpha[idx_best_alpha]\n",
    "print(\"chosen alpha : \", chosen_alpha)\n",
    "\n",
    "\n",
    "\n",
    "#training LR with best alpha found\n",
    "LR = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",\n",
    "                        penalty='l2', C=1./chosen_alpha)    \n",
    "LR.fit(embeddings[\"train\"],labels[\"train\"],)\n",
    "\n",
    "predictions = LR.predict(embeddings[\"dev\"])\n",
    "print(\"score on train : \", LR.score(embeddings[\"train\"],labels[\"train\"]))\n",
    "print(\"score on dev : \", LR.score(embeddings[\"dev\"],labels[\"dev\"]))\n",
    "\n",
    "predictions_probas = LR.predict_proba(embeddings[\"train\"])\n",
    "print(\"loss on train : \", log_loss(labels[\"train\"],predictions_probas))\n",
    "\n",
    "predictions_probas = LR.predict_proba(embeddings[\"dev\"])\n",
    "print(\"loss on dev : \", log_loss(labels[\"dev\"],predictions_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting some predictions on the Test Set\n",
      "#########################\n",
      "some sentences with predicted label 0\n",
      "\n",
      "made by jackasses for jackasses .\n",
      "\n",
      "flaccid drama and exasperatingly slow journey .\n",
      "\n",
      "godawful boring slug of a movie .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 1\n",
      "\n",
      "this one 's weaker than most .\n",
      "\n",
      "maybe it 's the star power of the cast or the redundant messages , but something aboul `` full frontal '' seems , well , contrived .\n",
      "\n",
      "just because it really happened to you , honey , does n't mean that it 's interesting to anyone else .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 2\n",
      "\n",
      "god help the poor woman if attal is this insecure in real life : his fictional yvan 's neuroses are aggravating enough to exhaust the patience of even the most understanding spouse .\n",
      "\n",
      "the writers , director wally wolodarsky , and all the actors should start their own coeducational fraternity : kappa rho alpha phi .\n",
      "\n",
      "earns its laughs from stock redneck ` types ' and from the many , many moments when we recognize even without the elizabethan prose , the play behind the thing .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 3\n",
      "\n",
      "though it is by no means his best work , laissez-passer is a distinguished and distinctive effort by a bona-fide master , a fascinating film replete with rewards to be had by all willing to make the effort to reap them .\n",
      "\n",
      "sensitive though not quite revelatory documentary .\n",
      "\n",
      "enduring love but exhausting cinema .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 4\n",
      "\n",
      "kinnear gives a tremendous performance .\n",
      "\n",
      "thurman and lewis are hilarious throughout .\n",
      "\n",
      "windtalkers celebrates the human spirit and packs an emotional wallop .\n",
      "\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "#Predicting the labels the Test Set\n",
    "predictions = LR.predict(embeddings[\"test.X\"])\n",
    "\n",
    "print(\"Plotting some predictions on the Test Set\")\n",
    "print(\"#########################\")\n",
    "\n",
    "for label in range(5):\n",
    "    print(\"some sentences with predicted label \"+str(label)+\"\\n\")\n",
    "    selection = np.random.choice(np.argwhere(predictions==label).flatten(),3)\n",
    "    for i in selection:\n",
    "        print(sentences[\"test.X\"][i])\n",
    "    print(\"#########################\")\n",
    "    \n",
    "np.savetxt('logreg_bov_y_test_sst.txt', predictions, fmt='%i', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chosen C : ', 6.0)\n",
      "('score on train : ', 0.5644387217605057)\n",
      "('score on dev : ', 0.43454545454545457)\n",
      "Plotting some predictions on the Test Set\n",
      "#########################\n",
      "some sentences with predicted label 0\n",
      "\n",
      "flaccid drama and exasperatingly slow journey .\n",
      "\n",
      "a trashy , exploitative , thoroughly unpleasant experience .\n",
      "\n",
      "unfortunately , the picture failed to capture me .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 1\n",
      "\n",
      "writer\\/director john mckay ignites some charming chemistry between kate and jed but , when he veers into sodden melodrama , punctuated by violins , it 's disastrous and kate 's jealous female friends become downright despicable .\n",
      "\n",
      "it represents better-than-average movie-making that does n't demand a dumb , distracted audience .\n",
      "\n",
      "splashes its drama all over the screen , subjecting its audience and characters to action that feels not only manufactured , but also so false you can see the filmmakers ' puppet strings .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 2\n",
      "\n",
      "these spiders can outrun a motorcycle and wrap a person in a sticky cocoon in seconds , but they fall short of being interesting or entertaining .\n",
      "\n",
      "a well acted and well intentioned snoozer .\n",
      "\n",
      "largely a for-fans artifact .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 3\n",
      "\n",
      "-lrb- davis -rrb- has a bright , chipper style that keeps things moving , while never quite managing to connect her wish-fulfilling characters to the human race .\n",
      "\n",
      "the whole damn thing is ripe for the jerry springer crowd .\n",
      "\n",
      "reinforces the often forgotten fact of the world 's remarkably varying human population and mindset , and its capacity to heal using creative , natural and ancient antidotes .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 4\n",
      "\n",
      "it 's one of the saddest films i have ever seen that still manages to be uplifting but not overly sentimental .\n",
      "\n",
      "an endlessly fascinating , landmark movie that is as bold as anything the cinema has seen in years .\n",
      "\n",
      "andy garcia enjoys one of his richest roles in years and mick jagger gives his best movie performance since , well , performance .\n",
      "\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "#Trying Multi-Class SVM with Gaussian Kernel\n",
    "#Small grid search for C penalty for misclassified data\n",
    "#Automatical choice for the variance parameter of the Gaussian Kernel\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "if False:\n",
    "    #to find best C (take much time)\n",
    "    set_C = np.linspace(1,10,10) #search\n",
    "    scores_dev = []\n",
    "\n",
    "    for C in set_C:\n",
    "\n",
    "        SVM = SVC(C=C,kernel='rbf',gamma='auto')        \n",
    "        SVM.fit(embeddings[\"train\"],labels[\"train\"])\n",
    "        scores_dev.append(SVM.score(embeddings[\"dev\"],labels[\"dev\"]))\n",
    "\n",
    "    plt.plot(set_C,scores_dev)\n",
    "    plt.xlabel(\"C (penalty for outlier / misclassified data)\")\n",
    "    plt.ylabel(\"classification score on dev set\")\n",
    "    plt.show()\n",
    "    \n",
    "    idx_best_C = np.argmax(scores_dev) \n",
    "    chosen_C = set_C[idx_best_C]\n",
    "else:\n",
    "    #best C was found equal to 6 with the grid search above\n",
    "    chosen_C = 6.0\n",
    "\n",
    "    \n",
    "#Fitting SVM with best C\n",
    "print(\"chosen C : \", chosen_C)\n",
    "\n",
    "SVM = SVC(C=chosen_C,kernel='rbf',gamma='auto')\n",
    "SVM.fit(embeddings[\"train\"],labels[\"train\"])\n",
    "print(\"score on train : \", SVM.score(embeddings[\"train\"],labels[\"train\"]))\n",
    "print(\"score on dev : \", SVM.score(embeddings[\"dev\"],labels[\"dev\"]))\n",
    "\n",
    "\n",
    "#Predicting the labels the Test Set\n",
    "predictions = SVM.predict(embeddings[\"test.X\"])\n",
    "\n",
    "print(\"Plotting some predictions on the Test Set\")\n",
    "print(\"#########################\")\n",
    "\n",
    "for label in range(5):\n",
    "    print(\"some sentences with predicted label \"+str(label)+\"\\n\")\n",
    "    selection = np.random.choice(np.argwhere(predictions==label).flatten(),3)\n",
    "    for i in selection:\n",
    "        print(sentences[\"test.X\"][i])\n",
    "    print(\"#########################\")\n",
    "    \n",
    "np.savetxt('svm_bov_y_test_sst.txt', predictions, fmt='%i', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "entire_text = \"\"\n",
    "for s in sets:\n",
    "    for sent in sentences[s]:\n",
    "        entire_text = entire_text + \" \" + sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('size of vocabulary provided to one_hot : ', 17837)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "size_vocab = len(np.unique(text_to_word_sequence(entire_text)))\n",
    "print(\"size of vocabulary provided to one_hot : \", size_vocab)\n",
    "\n",
    "encoded_text = one_hot(entire_text, size_vocab)\n",
    "\n",
    "processed_sentences = {}\n",
    "cpt = 0\n",
    "for s in sets:\n",
    "    processed_sentences[s] = []\n",
    "    for sent in sentences[s]:\n",
    "        l = len(text_to_word_sequence(sent))\n",
    "        processed_sentences[s].append(encoded_text[cpt:cpt+l])\n",
    "        cpt+=l\n",
    "        \n",
    "assert(cpt==len(encoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8543, 49), (8543, 5))\n",
      "((1100, 44), (1100, 5))\n",
      "(2209, 52)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "for s in sets:\n",
    "    processed_sentences[s] = pad_sequences(processed_sentences[s])\n",
    "    \n",
    "from keras.utils import to_categorical\n",
    "    \n",
    "x_train = processed_sentences[\"train\"]\n",
    "y_train = to_categorical(np.array(labels[\"train\"]),5)\n",
    "\n",
    "x_val = processed_sentences[\"dev\"]\n",
    "y_val = to_categorical(np.array(labels[\"dev\"]),5)\n",
    "\n",
    "x_test = processed_sentences[\"test.X\"]\n",
    "\n",
    "print(x_train.shape, y_train.shape)    \n",
    "print(x_val.shape, y_val.shape)    \n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonatan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.4, recurrent_dropout=0.4)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64 #64  # number of hidden units in the LSTM\n",
    "vocab_size = size_vocab  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(nhid, dropout_W=0.4, dropout_U=0.4))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          570784    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 595,941\n",
      "Trainable params: 595,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonatan/.local/lib/python2.7/site-packages/ipykernel_launcher.py:12: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8543 samples, validate on 1100 samples\n",
      "Epoch 1/6\n",
      "8543/8543 [==============================] - 8s 886us/step - loss: 1.5766 - acc: 0.2647 - val_loss: 1.5696 - val_acc: 0.2545\n",
      "Epoch 2/6\n",
      "8543/8543 [==============================] - 7s 824us/step - loss: 1.5424 - acc: 0.3055 - val_loss: 1.4915 - val_acc: 0.3464\n",
      "Epoch 3/6\n",
      "8543/8543 [==============================] - 7s 795us/step - loss: 1.3671 - acc: 0.3893 - val_loss: 1.4202 - val_acc: 0.3773\n",
      "Epoch 4/6\n",
      "8543/8543 [==============================] - 7s 770us/step - loss: 1.1778 - acc: 0.4596 - val_loss: 1.4414 - val_acc: 0.3609\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmYU+X1wPHvAYYdkVWRbVAQkOIGKghVChZwRUTZBRV/aNW21NYNq6Boq21t1bog6ogLAopSULSAyuKGOgoqKAUULOACIqiUbYDz++PccQLMkswkuUnmfJ4nzyQ3N8m5DubMu51XVBXnnHOuJBXCDsA551x68IThnHMuKp4wnHPORcUThnPOuah4wnDOORcVTxjOOeei4gnDOedcVDxhOOeci4onDOecc1GpFHYA8VS/fn3Nzs4OOwznnEsr77///req2qCk8zIqYWRnZ5Obmxt2GM45l1ZE5ItozvMuKeecc1HxhOGccy4qnjCcc85FJaPGMJxzrjTy8vJYt24dO3bsCDuUhKpatSpNmjQhKyurVK/3hOGcK/fWrVtHrVq1yM7ORkTCDichVJVNmzaxbt06WrRoUar38C6pCGPHhh2Bcy4MO3bsoF69ehmbLABEhHr16pWpFeUJI8Itt4QdgXMuLJmcLPKV9Ro9YQCqcN11dn/RIvjhh3Djcc65VFTuE8bYsVChAvzlL/a4c2eoXRsOOgh694bf/x5ycuCdd+DHH0MN1TmXobZs2cIDDzwQ8+vOOOMMtmzZkoCICieqmrQPS7SOHTtqaVd679kD4yqN5fgZY1m2jJ9uy5dDZJdfs2Zw1FHQrl3B7aijoGbN+FyDcy75Pv30U9q2bRvz68aOjc/Y55o1azjrrLNYunTpPsd3795NpUrxnZtU2LWKyPuq2rGk13rCyPf119CokfVPRdizB1avZp8kkp9Idu4sOK958wMTSdu2nkicSwelTRgiB3xllMrAgQOZMWMGrVu3Jisri6pVq1KnTh2WL1/OihUrOPfcc1m7di07duzgt7/9LSNHjgQKyiFt3bqV008/na5du/LWW2/RuHFjZsyYQbVq1Q74rLIkDJ9WC7BlC+RPM5swAQYMsH4poGJFaNnSbn36FLxkzx74/PMDE8mrr8KuXQXnZWcXnkhq1Eje5TnnojdqFCxZEv353bqVfM6xx8Lddxf9/B133MHSpUtZsmQJ8+fP58wzz2Tp0qU/TX/Nycmhbt26bN++nRNOOIF+/fpRr169fd5j5cqVTJ48mYcffpj+/fvz3HPPMXTo0OgvJAqeMMaO3Xd61GWX2a19e7jnHjj1VBvk2E/FitCqld3OPbfg+O7dhSeSV17ZN5G0aFF4IqlePXGX6pwruzVr4IuIUn0LFtjP5s3tD8R4OPHEE/dZK3Hvvfcyffp0ANauXcvKlSsPSBgtWrTg2GOPBaBDhw6sWbMmPsFE8IQR2QkpAu++a6PcTz8N3bvbN/vFF8Pw4TaAUYJKleDII+3Wt2/B8d274bPP9k0in3wCc+cWJBKRwhNJmzaeSJxLluJaAvuLV5fU/mpEdEHMnz+fV155hbfffpvq1avTrVu3QtdSVKlS5af7FStWZPv27XGPyxPG/k44wW533QXTp1vyuPlmGDMGTjsNLrnEmhRVq8b0tpUqQevWdjvvvILju3fDqlUHJpLZsyEvz84RgcMPLxhgj0wkhXRROufSTK1atfixiGmY33//PXXq1KF69eosX76cRYsWJTm6Ap4wIo0ZU3C/enUYMsRuq1fD44/DY4/BoEFw8MEweLAlj+OPt2/0UqpUyb7427SBfv0KjuflFZ5IXnrJkgxYT1lRiSTGfOacK4XIr4yyqFevHl26dOFnP/sZ1apV45BDDvnpud69ezN+/Hjatm1L69at6dSpU3w+tBR8llQs9u6F116zxPHcczZN6uijLXEMGQL16yfuswN5ebBy5YGJZMWKfRPJEUccmEhat/ZE4lxhSjtLKh35tNpAwhNGpM2bYcoUSx7vvQdZWXDOOZY8eva0pkMS7dpVdCLZs8fOqVDBZnsVlkgiuj+dK3c8YaTAtFoRyQHOAjao6s8Keb4bMANYHRx6XlVvDZ5bA/wI7AF2R3MxSVWnDvzqV3b7+GNLHE8+aS2PRo1skPzii230OwkqVy5IAJF27bKkEZlEli2DmTMLEkn+1OH9E8mRR3oicc4VSGgLQ0ROAbYCTxSTMP6gqmcV8twaoKOqfhvt5yW1hVGYXbtg1iwbKH/pJevC6trVEscFF0CtWuHFtp+dOwtPJKtW7ZtIWrXad0V7fiKpXDnc+J2LJ29hpEALQ1UXikh2Ij8jpVSubHNp+/aFL7+0Fsdjj8GIEfCb30D//tZl1aVLmQbK46FKFVtq0r79vsd37oT//GffRPLxxzZhbO9eO6dSpcITSatWnkicy2QJH8MIEsaLxbQwngPWAV9irY1lwXOrgc2AAg+p6oQi3n8kMBKgWbNmHb6IXFGTClTh7bet1TF1Kmzdat+sF18Mw4ZB48ZhRxiVHTsOTCTLltnakshEcuSRhSeSUm7w5VxSeAsjRQa9S0gYBwF7VXWriJwB3KOqrYLnGqvqehFpCMwFfq2qC4v7rNC7pEqydauNceTkwMKFNgrdu7e1Os4+Oy3/PN++vehEkv9PKyur8ETSsmXJiSRexd2cK44njDRIGIWcu4ZCxi1EZCywVVX/VtzrUz5hRFq5EiZOtPUd69dDvXowdKglj6OPDju6Mtu+3Qo07p9IPv9830TSuvWBlX9btiyYZJaolbTORUq3hFGzZk22bt1aqtem7BhGSUTkUOAbVVURORHbn2OTiNQAKqjqj8H9nsCtYcYad61awe23w623Wn2QnBx48EGrX9Whg3VZDR5ss7HSULVqcNxxdou0bVtBIslPIu+9B888U5AYKlcuSCRgx8vBZmguHZWzJnCiZ0lNBroB9YFvgDFAFoCqjheRq4BfAbuB7cDVqvqWiBwOTA/ephLwtKreXtLnpVULozCbNlkNq0cfhQ8/tJHpvn2t1dG9u01bylDbtsGnn1oCmTAB3nzzwHPGjClX/2+6JCp1CyNOTeDrr7+epk2bcuWVVwIwduxYKlWqxLx589i8eTN5eXncdttt9AlKZofVwvCFe6lq8WJrdUyaZIsEmzaFiy6y2+GHhx1dUqgWFAru3h2mTUvbBpdLcft8icZS33zBAqtoXZIS6psvXryYUaNGsSAofXvUUUcxe/ZsateuzUEHHcS3335Lp06dWLlyJSISWsIo91u0pqzjjoN//tOm506dap37t91mNT+6d7cpu9u2hR1lQuV3Qz3xBLz+Opx8so2BOBeqNWssUeTXNc+/X4Zy4scddxwbNmzgyy+/5MMPP6ROnToceuihjB49mqOPPprTTjuN9evX880338TlEkrLiw+muqpVbf1G//6wdq19e+bk2JTcq66CgQNtvOOkkzKyo3/MGLjwQttroG9f6NQJZsywvdedS4iQ6ptfcMEFTJs2ja+//poBAwYwadIkNm7cyPvvv09WVhbZ2dmFljVPJm9hpJOmTeHGG22G1fz59g361FP27dmuHfztbxDyXyDxlj9mccoptpyldm34xS+s0eVcJhkwYABTpkxh2rRpXHDBBXz//fc0bNiQrKws5s2bRyqsMfOEkY4qVLB+04kT4auv4OGHreT6NdfYQsA+fezP8PwNNTLEkUda0jjhBGtY/elPPuXWhSxe9c2Bdu3a8eOPP9K4cWMaNWrEkCFDyM3NpX379jzxxBO0adMmbp9VWj7onUmWL7dSJI8/bi2Nhg2t6+rii20MJEPs3GnVViZNsjkADz2UlmseXQpJt3UYZeGD3s60aQN33mljHTNnWs2qu++27qpOnWy+6vffhx1lmVWpYmP+Y8daI6t3b5tI5pxLLE8YmSgry0qNPP+8rSK/6y4rS3LZZVZ6/cILYd68giJQaUjEegOefNLWbPgMKucSzxNGpmvYEK6+2krOvvuu7dMxc6ZNzW3ZEsaNg//+N+woS23oUHjlFdiwwSaKvfVW2BG5dJVJ3fNFKes1esIoL0RstPjBB+Hrr20A4PDD4eabITvbdgmcMsXK0qaZn/8cFi2yRX3du9tlOBeLqlWrsmnTpoxOGqrKpk2bqFqGfZp90Lu8W7PGBgImToQvvrDZVoMHWzmS449Pq7UdmzbZTOPXX7c1jqNHp1X4LkR5eXmsW7cu9HUOiVa1alWaNGlC1n5lor00iIvN3r02rpGTY2MfO3ZY1dxLLoEhQ6B+/bAjjMrOnXDppbY8ZfhwG+f3GVTOFc9nSbnYVKgAPXpYV9VXX1nXVZUqVlfnsMPg/PNt29ndu8OOtFhVqthi+LFjbXZxr17w3XdhR+VcZvCE4Q508MFw+eU2SP7RR1aCZOFCOPNMaNYMbrjBNgRPUfkzqJ56ygbBTz7ZNnRyzpWNJwxXvPbt4e9/h3XrrKuqQwf4y19sw4qf/9y6sH78MewoCzVkiM2g+vZbW4ZSWMl051z0PGG46FSubCPKL7xgyePOO2HjRlty3aiRjXW88UbK1eqInEHVowdMnhx2RM6lL08YLnaNGsG119qOR2++CYMGwbPP2rdz69bw5z/bgsEU0bKl1aA66SSbAHbbbSmX15xLCwlNGCKSIyIbRGRpEc93E5HvRWRJcLs54rneIvIfEVklItcnMk5XSiI2QPDww7a2Y+JESyajR9tYx5ln2q5HO3eGHSn16sGcObbI/aabrLzWrl1hR+Vcekl0C2Mi0LuEc15X1WOD260AIlIRuB84HTgKGCQimVM9LxPVqGHzWBcssPLrN9xg28xecIFV0B01yh6HqEoVmzl1yy32s2dPn0HlXCwSmjBUdSFQmv8lTwRWqernqroLmAL0iWtwLnFatrR+ny++gJdftsGDBx+0bSo7dID77w+tWqCILW6fNMm6qTp3hlWrQgnFubSTCmMYnUXkQxF5WUTaBccaA2sjzlkXHDuAiIwUkVwRyd24cWOiY3WxqFjRSslOnWpbzd57ry0QvOoq67oaNMj6ifbsSXpogwfDq6/a6nCfQeVcdMJOGB8AzVX1GOCfwL9ifQNVnaCqHVW1Y4MGDeIeoIuTevXg17+GxYvhgw9g5EiYPdtW1rVoYX/2F1ZuNn/LvQTo2tVmUNWrZzWofAaVc8ULNWGo6g+qujW4/xKQJSL1gfVA04hTmwTHXCY47jhrbXz5pbU+2rWzLqwjjrD9V598ErZts3NvuSWhoeTPoOrc2Vod48b5DCrnihJqwhCRQ0WsPJyInBjEswl4D2glIi1EpDIwEJgZXqQuIapWhf79bZzjiy8saaxda7sEHnqotUIg4d/gdetaz9iwYdbQueiilJjY5VzKSfS02snA20BrEVknIiNE5HIRuTw45XxgqYh8CNwLDFSzG7gKmA18CjyjqssSGasLWdOmcOONNsPqoots9fjDD9tzFSrYaPWvfpWwj69c2WYFjxtntah8BpVzB/JqtS51/fAD1K5t4xyvvGKD4+3a2WD5wIHWhZUAkydbzmre3OottmyZkI9xLmV4tVqX/g46yH7++9823nH//Vbj449/tG/xk06yPcu/+iquHztoELz2mrUwOnWyiifOOU8YLtWNGWM/GzaEK66w3ZG++MIKIOblwe9+ZwsDu3e3Lqw49SN16VIwg6pHD3j66bi8rXNpzbukXHpbvtz2ZH36aRv/yMqyLqzBg+Gcc2wFehl89x306wfz59uErZtu8l38XObxLilXPrRpY2s1/vMfyM2F3/zG1noMHmytkkGDYObMUk97qlvXlosMH26NneHDfQaVK788YbjMIGJlR/72N/jvf62m1bBhMHcu9Olj03QvvdSWd8e4srxyZXjsMZv1++STNoNq06YEXYdzKcy7pFxmy8uzGVaTJ8P06bB1qyWP/v2t9XHSSTH1MU2ZYjOomjWDWbOgVavEhe5csniXlHNgYxqnn26LKzZsgGeesWXdDz1kP484wtZ/LC20Av8BBg60GVSbN9sMqtdfT3D8zqUQTxiu/KhWzcqtP/88fPON9TO1amW7B7Zvb7c//anwmlYRTj7ZZlA1aACnnWZ7hztXHnjCcOVT7drWtzR7tq3xuO8+O3bjjdbq6NQJ7rmnyDUeRxxhNahOPtk2ZbrlFq9B5TKfJwznGjaEK6+0FXpr1liLY+dO2/SpSRNbiPHIIwfs4VGnjuWbiy6yiVrDhvkMKpfZok4YItJFRGoE94eKyN9FpHniQnMuBM2b237lixfDJ5/YqvK1a+H//g8OOcTWdkyZAv/7H2AzqHJy4PbbrWvql7/0GVQuc8XSwngQ2CYixwC/Bz4DnkhIVM6lgrZtra8pf43Hr39te3kMGmStksGD4YUXkLxdjB5teeTdd20sfeXKsIN3Lv5iSRi71ebg9gHuU9X7gVqJCcu5FJK/xuOuu2yNx/z5NnAxZ461OA49FP7v/xjQ4DVem7vnpxlUCxeGHbhz8RVLwvhRRG4AhgKzRKQCkJWYsJxLURUqwKmnwvjxNiA+axaceaY1L3r04OQBTVl11ihOq/UOp/VQn0HlMkosCWMAsBMYoapfY7vg/TUhUTmXDrKy4IwzbPn3N9/YGo9Onaj99INM/aITayq1ZM2Ff+T+K5b5DCqXEaJe6R0MeO9Q1T0iciTQBnhZVfOKeU0OcBawQVV/Vsx5J2AbLQ1U1WnBsT3Ax8Ep/1XVc0qK0Vd6u5SwZQtMn87eSZPhtVepoHv578HtOezqQVQaOtD2MHcuhSRipfdCoIqINAbmABcCE0t4zUSgd3EniEhF4M7gPSNtV9Vjg1uJycK5lHHwwXDxxVR4ZQ7y5ZfMOfufrN1Si0o3j4bDD7dR8Xvvha+/DjtS52ISS8IQVd0GnAc8oKoXAEW2GgBUdSFQ0gYFvwaeAzbEEItzaUEOPYSeM69i3ZQ3aV15NX+tdwc7tmyH3/7W9vE47TSbl7tlS9ihOleimBKGiHQGhgCzSvH6wt6wMdAXm7K7v6oikisii0Tk3LJ8jnNhGzAAJs7P5q8VrqPxhiXkPr7MVpV/8QWMGGFrPM49F6ZOhW3bwg7XuULF8oU/CrgBmK6qy0TkcGBeGT//buA6Vd1byHPNgz61wcDdIlLoBs4iMjJILLkbN24sYzjOJU7nzlaDqmFDOPnSo3iy1a2wYoUt3rjySnjvPatu2LAhDBkCL74Iu3aFHbZzP4m5vLmI1ARQ1a1Rnp8NvFjYoLeIrAbya0vXB7YBI1X1X/udNzF4j2nFfZYPert0sGWL7eL32mtw881WVkQE26fj9detFPu0abbdX926dvKgQXDKKVCxYtjhu1Q1dqzdSiHaQe9YZkm1x1Z218W+5DcCw1R1WQmvy6aIhLHfeROD86aJSB1gm6ruFJH62AyqPqr6SXHv4QnDpYtdu+BXv7Lhi8GD4dFHoWrV/U6YO9eSx7/+ZaVIDjvM9vEYPBg6dvS9Yt2+REpdATPahFEphvd8CLhaVecFH9ANeBg4uZggJgPdgPoisg4YQ7DYT1XHF/NZbYGHRGQv1m12R0nJwrl0Urmy1TNs2RJGj7YF5NOnQ/36ESeceabdtm2z7qnJk+GBB+Duu61c7qBBdjvqqFCvxYVg1y5bOPrll7B+vf1MglhaGB+q6jElHQuTtzBcOnrmGat026QJvPQSHHlkMSdv2WL7eUyebH1ae/fC0Udb4hg4ELKzkxW2S4S9e+Hbb/dNBIX9LGm8dsyYmLqnEtElNR34AHgyODQU6KCqfaOOKsE8Ybh0tWiRlaXavdtaGqeeGsWLvv4ann3Wksfbb9uxzp0tefTvbzOvXOr48cfik8CXX1qrIW+/tdAiNhHisMNsKnZRPxs0SHiXVCwJow5wC9A1OPQ6MFZVNxf9quTyhOHS2erV1gO1apV1Vw0bFsOL16yxelaTJ8NHH1nNq+7dbbyjb19bTOgSo7DuocJ+bi1kntBBB5WcCA491MrQlCQJYxgxz5JKZZ4wXLrbsgXOPx9efRVuusmqq8c8tv3JJ5Y4nn7atputXNlqXg0aBGedBdWrJyT2jFOW7qHKle0Lv7hkcNhhULNm/OJNhVlSIvICUORJqVS2wxOGywR5eTaD6tFH7Ts+J2e/GVTRUrW1HZMn24LAr76yL6g+feyNe/aM7i/XTJTo7qF69dJqFls8E0axvamquiDG2BLGE4bLFKrwl7/A9ddDly42rtGgQRnecM8e26Ajf43H5s22xuP88wvWeFTIgB2by9o9VFwSOOyw6LuH0kzSu6RE5DlV7ReXNyslTxgu00ybZns1NW5sW2+0bh2HN921yzZ/mjwZZswoWOMxcKAljw4dUu+v43TrHkozYSSMxap6XFzerJQ8YbhMtGiR9SLl5dmM2m7d4vjm//tfwRqPl1+2ZNKyZcEaj7Zt4/hhRShr91Dkl34GdA+FIYyE8YGqHh+XNyslTxguU5VpBlW0Nm+2vq+nn4Z58+yv+mOOKVjj0bx5wbnRDLB691Da8IThXIaJnEH1xz/Crbcm8A/nr7+2FYWTJ1sTB+Dkky15nH8+NGoEixd791CG8C4p5zJQXh5ccYW1MgYOhMceK+UMqlisXl2wxuPjjws/Z//uoaJ+evdQSop7LSkRORuYVUQpcoDron0v51zpZGXBhAnQqhVcd53VoPrXv8o4g6okLVrADTfAzp2FJ4xRo2xKl3cPZbxYVno/BXTGdsfLUdXliQysNLyF4cqT556DoUPtD/dZs6BNmyR+eBlWFbvUE/c9vVV1KHAc8BkwUUTeDjYvqlWGOJ1zpdSvH8yfb2PGnTvbOLVziRTTSh1V/QGYBkwBGmHbq34gIr9OQGzOuRKcdBK88461Mnr2hIkTk/TBY8Yk6YNcKok6YYjIOUHF2vnYnhYnqurpwDHA7xMTnnOuJNnZ8Oabtj7j4ottBtXeokYa46WUNYtceotlA6V+wD9UdWHkQVXdJiIj4huWcy4WBx9se2lceSXcfrut15g4MQkzqFy5EssYxnBgRdDSOFtEDo147tXCXiMiOSKyQUSWFvfeInKCiOwWkfMjjg0XkZXBbXi0cTpXXmVlwUMP2YSlqVOtunlJ++w4F4tYuqRGAO8C5wHnA4tE5JISXjYR6F3C+1YE7gTmRByri23nehJwIjAm2I/DOVcMEbjmGqtBtXgxdOoEy1NuPqNLV7EMel8LHKeqFwWtjQ6UsPYi6L76roT3/TU2VXdDxLFewFxV/S7YoGkuJSQe51yBfv1gwQIrFdW5s+3m6lxZxZIwNgE/Rjz+MThWaiLSGJtp9eB+TzUG1kY8Xhccc85F6cQTbQZV48bQq5etCneuLGIZ9F4FvCMiM7ANlfoAH4nI1QCq+vdSfP7dwHWquldKWS5AREYCIwGaNWtWqvdwLlM1b24zqC64AC65xAbDx43LjK0vXPLFkjA+C275ZgQ/y7JwryMwJUgW9YEzRGQ3sB7oFnFeE2w67wFUdQIwAWyldxlicS4j1a5tK8Gvugr+9Cf47DNrbVSrFnZkLt1EnTBU9RYAEakZPC6kJnFsVLVF/n0RmQi8qKr/Cga9/xQx0N0TuKGsn+dceZWVBePHWw2qa6+1GlQzZiS4BpXLOLHMkvqZiCwGlgHLROR9EWlXwmsmA28DrUVknYiMEJHLReTy4l6nqt8B44D3gtutwTHnXCmJwB/+YDOoliyxVeKffhp2VC6dxFJ88C3gRlWdFzzuBvxJVU9OXHix8eKDzkXnvffg7LNhxw7bxa9797AjcmGKe/FBoEZ+sgBQ1flAjVLE5pwL2Qkn2Ayqpk1tBlVOTtgRuXQQS8L4XERuEpHs4PZH4PNEBeacS6zmzeGNN6x1MWIEjB6dhBpULq3FkjAuARoAz2ML7eoHx5xzaap2bXjxRbjsMvjzn20Xv+3bw47KpaqoZkkF5TtuVNXfJDge51ySZWXBgw/aDKprroG1a20GVcOGYUfmUk1ULQxV3QN0TXAszrmQiMDvf2+7+H34odWg8hlUbn+xdEktFpGZInKhiJyXf0tYZM65pOvb12pQbd9uNaheLbQOtSuvYkkYVbHaUd2Bs4PbWYkIyjkXnsgZVL17w6OPhh2RSxWxlAZ5RFXfjDwgIl3iHI9zLgU0a2Y1qPr3h0svtRpUt9/uNajKu1h+/f+M8phzLgMcdJDNoLr8crjjjn1nUPkOreVTiS0MEekMnAw0yK9MGzgIqJiowJxz4atUCR54wGZQ/eEPVoNq5ky45RZPGuVRNF1SlYGawbmRlWl/wHbec85lMBG4+mo4/HAYPNhqULnyqcSEoaoLgAUiMlFVv0hCTM65FLRkiXVJrVljj/O3sBkzxlsb5UUsYxhVRGSCiMwRkdfybwmLzDmXUsaOBVVYudIe16xpW796sig/Ypkl9SwwHngE2JOYcJxzqa5lS/uZnW3TbqdOhXPPDTUklySxtDB2q+qDqvquqr6ff0tYZM65lDVmjC3w69AB+vXz/cLLi1gSxgsicoWINBKRuvm3hEXmnEtZY8dC3bowdy788pe2X/hdd4UdlUu0WBLGcOAa4C3g/eBW7G5FIpIjIhtEZGkRz/cRkY9EZImI5IpI14jn9gTHl4jIzBjidM4lSY0aNs12wACbdnvDDTbO4TJTLHt6tyj5rANMBO4Dniji+VeBmaqqInI08AzQJnhuu6oeW4rPdM4lUeXKMGkS1KljC/w2bbLqtxV9lVbGiTphiEh14GqgmaqOFJFWQGtVfbGo16jqQhHJLub5rREPawD+t4lzaahiRVvgV78+3HYbbN4MTz0FVaqEHZmLp1i6pB4DdmGrvgHWA7eVNQAR6Ssiy4FZ7LshU9Wgm2qRiPgcDOdSnAiMGwf/+AdMm2Z7hm/dWvLrXPqIJWEcoap/AfIAVHUbIGUNQFWnq2ob4FxgXMRTzYNNyQcDd4vIEYW9XkRGBokld+PGjWUNxzlXRqNGwcSJtkajRw/ronKZIZaEsUtEqhF0GwVf4DvjFYiqLgQOF5H6weP1wc/PgfnAcUW8boKqdlTVjg0aNIhXOM65Mhg+HJ5/3jZjOuUUWL8+7IhcPMSSMMYA/waaisgkbMD62rJ8uIiZqESXAAAS8ElEQVS0FLECAyJyPFAF2CQidUSkSnC8PtAF+KQsn+WcS65zzoHZs23L1y5dYMWKsCNyZRXLLKm5IvIB0Anrivqtqn6b/7yItFPVZZGvEZHJQDegvoisw5JOVvB+44F+wDARyQO2AwOCGVNtgYdEZC+W1O5QVU8YzqWZU0+F+fNtRXjXrpZAjiu0r8ClA9E4TZoWkQ9U9fi4vFkpdezYUXNzi10a4pwLwYoVtsBvyxZ44QXrpnKpQ0TeD8aMixXP/bPKPADunMtMRx5pO/g1bgy9elnScOknngnD11A454rUpAksXAjt20PfvvDkk2FH5GLlO/Q655Kmfn149VXo1g2GDYN77gk7IheLeCaMXXF8L+dchqpVC2bNsiq3o0bBzTd7/al0EXXCEJEuIlIjuD9URP4uIs3zn1fVTokI0DmXeapUsX00Royw1eFXXQV794YdlStJLC2MB4FtInIM8HvgM4ouKuicc8WqWBEefhiuvdbqUA0ZAru8nyKlxbLj3u5gjUQf4D5VfVRERiQqMOdc5hOBO++EevXguuts2u20aVY23aWeWFoYP4rIDcBQYJaIVCBYhOecc2Vx7bXwyCMwZw707GnVbl3qiSVhDMBqR41Q1a+BJsBfExKVc67cGTECnn0WcnNtYd9XX4UdkdtfTC0M4B5VfV1EjgSOBSYnJiznXHl03nnw0kuwZo3Vn/rss7AjcpFiSRgLgSoi0hiYA1yI7ajnnHNx06OHlUb/4QerP/XRR2FH5PLFkjAk2APjPOABVb0A+FliwnLOlWcnnACvvw6VKlkBwzffDDsiBzEmDBHpDAzBdseL9fXOORe1tm0tUTRsaIULX3457IhcLF/4o4AbgOmqukxEDgfmJSYs55yDZs3gjTcseZxzDkz2UdNQxbIfxgJggYjUFJGawU54v0lcaM45Bw0awLx5ljCGDLEpt1dcEXZU5VMspUHai8hiYBnwiYi8LyLtEheac86Zgw6Cf/8bzj4brrwSbr3V60+FIZYuqYeAq1W1uao2w8qDPFzcC0QkR0Q2iMjSIp7vIyIficgSEckVka4Rzw0XkZXBbXgMcTrnMlDVqvDcc7Zf+JgxVrjQ608lVyylQWqo6k9jFqo6P78YYTEmAvdRdM2pV4GZQcmRo4FngDYiUhfbzrUjts/G+yIyU1V9/adz5VilSpCTA3Xrwj/+Ad99Z4+zvOZEUsSSMD4XkZuA/G1PhgKfF/cCVV0oItnFPL814mENCjZh6gXMVdXvAERkLtAbXyjoXLlXoQLcdZftrXHjjVZ/6plnoFq1sCPLfLF0SV0CNACeB54D6gfHykRE+orIcmyqbv77NQbWRpy2LjhW2OtHBt1ZuRs3bixrOM65NCACo0fD+PG2t0avXpY4XGJFlTBEpCJwo6r+RlWPV9UOqjoqHl1EqjpdVdsA5wLjSvH6CaraUVU7NmjQoKzhOOfSyGWXwZQpsGgR/OIX8M03YUeU2aJKGKq6B+ha4olloKoLgcNFpD6wHmga8XST4Jhzzu2jf3944QVYscJKiaxZE3ZEmSuWLqnFIjJTRC4UkfPyb2X5cBFpKSIS3D8eqAJsAmYDPUWkjojUAXoGx5xz7gC9etle4Zs2WdHCZcvCjigzxTLoXRX7Mu8ecUyxMY1CichkoBtQX0TWYTOfsgBUdTzQDxgmInnAdmCAqirwnYiMA94L3urW/AFw55wrTKdOsHCh7adxyilW9fakk8KOKrOIRrn6RUQeB36rqluCx3WAu1S1zAPf8dKxY0fNzc0NOwznXIhWr7baU19/DdOn231XPBF5X1U7lnReLF1SR+cnC4BgwPu40gTnnHOJ0qKF1Z9q2RLOPNM2ZXLxEUvCqBC0KgAIFtfF0qXlnHNJceihMH++dUkNGAATJoQdUWaI5Qv/LuBtEcnP1xcAt8c/JOecK7uDD4bZs20W1WWX2YD49dfbGg5XOrFUq31CRHIpGPQ+T1U/SUxYzjlXdtWr2zjGxRfbQr9Nm+Cvf/WkUVoxdSkFCcKThHMubWRlwRNPWP2pu+6ypPHww1aXysXG/5M55zJehQpwzz1Wf2rMGNtTY8oUq4DroudbrDrnygURuPlm+Oc/YcYMOOMM+OGHsKNKL54wnHPlylVXwaRJ8Prr0L07eM3S6HnCcM6VO4MHWyvjk0/g5z+H//437IjSgycM51y5dMYZMGeOrQjv0gWWLw87otTnCcM5V2517QoLFkBent33ykLF84ThnCvXjjnGSonUqmV7arz2WtgRpS5PGM65cq9lS3jzTcjOhtNPt8V+7kCeMJxzDjjsMOue6tABzj8fcnLCjij1eMJwzrlA3bowd66VRB8xAv72t7AjSi0JTRgikiMiG0RkaRHPDxGRj0TkYxF5S0SOiXhuTXB8SVDDyjnnEq5GDZg506rcXnONFSyMctugjJfo0iATgfuAJ4p4fjVwqqpuFpHTgQlA5B5Zv1DVbxMbonPO7atyZVvcV6cO3Hmn1Z8aPx4qVgw7snAlNGGo6kIRyS7m+bciHi4CmiQyHueci1bFivDAA1Z/6rbbYMsWeOopqFIl7MjCk0rFB0cAL0c8VmCOiCjwkKr6FijOuaQSgXHjoF49+N3v4Pvv4fnnoWbNsCMLR0okDBH5BZYwukYc7qqq60WkITBXRJar6sJCXjsSGAnQrFmzpMTrnCtfRo2yAfFLLoEePeCllyyJlDehz5ISkaOBR4A+qrop/7iqrg9+bgCmAycW9npVnaCqHVW1Y4MGDZIRsnOuHBo2zFoXH34Ip5wC69eHHVHyhZowRKQZ8DxwoaquiDheQ0Rq5d8HegKFzrRyzrlkOecc2/Z17VqrP7ViRcmvySSJnlY7GXgbaC0i60RkhIhcLiKXB6fcDNQDHthv+uwhwBsi8iHwLjBLVf+dyFidcy4ap54K8+fDtm1Wf2rx4rAjSh7RDJpg3LFjR8316mHOuSRYscIW+G3ZAi+8YN1U6UpE3lfVjiWdF/oYhnPOpaMjj7T6U40bQ69eljQynScM55wrpSZNYOFCaN8e+vaFJ4paopwhPGE451wZ1K8Pr74K3brB8OFw991hR5Q4njCcc66MatWCWbOgXz9b4HfTTZlZf8oThnPOxUGVKjB1Klx6qZUSufJK2Ls37KjiKyVWejvnXCaoWBEmTLBV4HfeCZs3w+OPWzHDTOAJwznn4kgE7rjDksa119q022nTrGx6uvMuKeecS4BrroFHHoE5c6BnT2ttpDtPGM45lyAjRsCzz0Juri3s++qrsCMqG08YzjmXQOedZ9Vt16yx+lOffRZ2RKXnCcM55xKsRw947TX44QerP/XRR2FHVDqeMJxzLglOOAFefx0qVbLuqTffDDui2HnCcM65JGnb1hLFIYdY4cKXXgo7oth4wnDOuSRq1gzeeMOSR58+8PTTYUcUPU8YzjmXZA0awLx5Np4xdCjcf3/YEUXHE4ZzzoXgoIPg5ZdtF7+rroJbb039+lOJ3nEvR0Q2iEih26uKyBAR+UhEPhaRt0TkmIjneovIf0RklYhcn8g4nXMuDFWr2irwiy6CMWNg1KjUrj+V6NIgE4H7gKKqxK8GTlXVzSJyOjABOElEKgL3A78E1gHvichMVf0kwfE651xSVaoEjz4KderAP/4B330HOTmQlRV2ZAdKaMJQ1YUikl3M829FPFwENAnunwisUtXPAURkCtAH8IThnMs4FSrAXXfZ3ho33mj1p555BqpVCzuyfaXSGMYI4OXgfmNgbcRz64JjzjmXkURg9GgYP9721ujVyxJHKkmJhCEiv8ASxnWleO1IEckVkdyNGzfGPzjnnEuiyy6DKVNg0SLbxe+bb8KOqEDoCUNEjgYeAfqo6qbg8HqgacRpTYJjB1DVCaraUVU7NmjQILHBOudcEvTvDy+8ACtX2tTbNWvCjsiEmjBEpBnwPHChqq6IeOo9oJWItBCRysBAYGYYMTrnXBh69bK9wjdtsqKFSwuda5pciZ5WOxl4G2gtIutEZISIXC4ilwen3AzUAx4QkSUikgugqruBq4DZwKfAM6q6LJGxOudcqunUCRYutPUZp5xi3VRhEk31lSIx6Nixo+bm5oYdhnPOxdXq1bYJ05dfwvTpdj+eROR9Ve1Y0nmhj2E455wrXosWVn+qVSs46yybchsGTxjOOZcGDjkE5s+Hk06CgQNhwoTkx+AJwznn0sTBB8Ps2XDGGTb99s9/Tm79KU8YzjmXRqpXt3GMIUNsod8111jSGDs28Z+d6FpSzjnn4iwrC554AurWtZIimzbBxImJTxqeMJxzLg1VqAD33GP1p8aMsWO7d1sxw4R9ZuLe2jnnXCLdcktBsgBreYgkrqXh6zCccy4DiJR+ANzXYTjnnIsrTxjOOZcBIrumEsUThnPOZYBkTKv1hOGccy4qnjCcc85FxROGc865qHjCcM45FxVPGM4556KSUQv3RGQj8EUZ3qI+8G2cwglTplwH+LWkqky5lky5DijbtTRX1QYlnZRRCaOsRCQ3mtWOqS5TrgP8WlJVplxLplwHJOdavEvKOedcVDxhOOeci4onjH2FsOlhQmTKdYBfS6rKlGvJlOuAJFyLj2E455yLircwnHPORaXcJQwR6S0i/xGRVSJyfSHPVxGRqcHz74hIdvKjjE4U13KRiGwUkSXB7dIw4iyJiOSIyAYRWVrE8yIi9wbX+ZGIHJ/sGKMVxbV0E5HvI34nNyc7xmiISFMRmScin4jIMhH5bSHnpMXvJcprSZffS1UReVdEPgyu5ZZCzkncd5iqlpsbUBH4DDgcqAx8CBy13zlXAOOD+wOBqWHHXYZruQi4L+xYo7iWU4DjgaVFPH8G8DIgQCfgnbBjLsO1dANeDDvOKK6jEXB8cL8WsKKQf19p8XuJ8lrS5fciQM3gfhbwDtBpv3MS9h1W3loYJwKrVPVzVd0FTAH67HdOH+Dx4P40oIeISBJjjFY015IWVHUh8F0xp/QBnlCzCDhYRBolJ7rYRHEtaUFVv1LVD4L7PwKfAo33Oy0tfi9RXktaCP5bbw0eZgW3/QeiE/YdVt4SRmNgbcTjdRz4D+enc1R1N/A9UC8p0cUmmmsB6Bd0F0wTkabJCS3uor3WdNE56FJ4WUTahR1MSYIujeOwv2Yjpd3vpZhrgTT5vYhIRRFZAmwA5qpqkb+XeH+HlbeEUd68AGSr6tHAXAr+6nDh+QArw3AM8E/gXyHHUywRqQk8B4xS1R/CjqcsSriWtPm9qOoeVT0WaAKcKCI/S9Znl7eEsR6I/Cu7SXCs0HNEpBJQG9iUlOhiU+K1qOomVd0ZPHwE6JCk2OItmt9bWlDVH/K7FFT1JSBLROqHHFahRCQL+4KdpKrPF3JK2vxeSrqWdPq95FPVLcA8oPd+TyXsO6y8JYz3gFYi0kJEKmMDQjP3O2cmMDy4fz7wmgajRymmxGvZrz/5HKzvNh3NBIYFs3I6Ad+r6ldhB1UaInJofn+yiJyI/T+Ycn+QBDE+Cnyqqn8v4rS0+L1Ecy1p9HtpICIHB/erAb8Elu93WsK+wyrF403SharuFpGrgNnYLKMcVV0mIrcCuao6E/uH9aSIrMIGLweGF3HRoryW34jIOcBu7FouCi3gYojIZGyWSn0RWQeMwQbzUNXxwEvYjJxVwDbg4nAiLVkU13I+8CsR2Q1sBwam6B8kXYALgY+D/nKA0UAzSLvfSzTXki6/l0bA4yJSEUtqz6jqi8n6DvOV3s4556JS3rqknHPOlZInDOecc1HxhOGccy4qnjCcc85FxROGc865qHjCcC5kQaXUF8OOw7mSeMJwzjkXFU8YzkVJRIYGexEsEZGHgiJwW0XkH8HeBK+KSIPg3GNFZFFQ+HG6iNQJjrcUkVeCIncfiMgRwdvXDApELheRSRGrju8I9nH4SET+FtKlOwd4wnAuKiLSFhgAdAkKv+0BhgA1sBW27YAF2MpugCeA64LCjx9HHJ8E3B8UuTsZyC+lcRwwCjgK2+Oki4jUA/oC7YL3uS2xV+lc8TxhOBedHljxxveC8hI9sC/2vcDU4JyngK4iUhs4WFUXBMcfB04RkVpAY1WdDqCqO1R1W3DOu6q6TlX3AkuAbKws9Q7gURE5Dyu/4VxoPGE4Fx0BHlfVY4Nba1UdW8h5pa21szPi/h6gUrCXwYnYJjhnAf8u5Xs7FxeeMJyLzqvA+SLSEEBE6opIc+z/ofODcwYDb6jq98BmEfl5cPxCYEGw29s6ETk3eI8qIlK9qA8M9m+oHZTb/h1wTCIuzLlolatqtc6Vlqp+IiJ/BOaISAUgD7gS+B+2ic0fsR3QBgQvGQ6MDxLC5xRUcr0QeCioLpoHXFDMx9YCZohIVayFc3WcL8u5mHi1WufKQES2qmrNsONwLhm8S8o551xUvIXhnHMuKt7CcM45FxVPGM4556LiCcM551xUPGE455yLiicM55xzUfGE4ZxzLir/DwA1IPIrRZLSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience = 0)\n",
    "        \n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=bs, nb_epoch=n_epochs, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[early_stopping_monitor])\n",
    "\n",
    "losses = {}\n",
    "losses[\"train\"] = history.history['loss']\n",
    "losses[\"val\"] = history.history['val_loss']\n",
    "epochs = np.arange(len(losses['train']))\n",
    "plt.plot(epochs,losses['train'],marker=\"+\",color=\"b\",label=\"train\")\n",
    "plt.plot(epochs,losses['val'],marker=\"+\",color=\"r\",label=\"val\")\n",
    "plt.xlabel(\"epochs\") ; plt.ylabel(\"cross_entropy_loss\") ; plt.legend()\n",
    "plt.savefig('q4_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting some predictions on the Test Set\n",
      "#########################\n",
      "some sentences with predicted label 0\n",
      "\n",
      "so i just did .\n",
      "\n",
      "the performances are so leaden , michael rymer 's direction is so bloodless and the dialogue is so corny that the audience laughs out loud .\n",
      "\n",
      "can see where this dumbed-down concoction is going .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 1\n",
      "\n",
      "the premise is in extremely bad taste , and the film 's supposed insights are so poorly thought-out and substance-free that even a high school senior taking his or her first psychology class could dismiss them .\n",
      "\n",
      "between the drama of cube ?\n",
      "\n",
      "it settles for being merely grim .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 2\n",
      "\n",
      "this pep-talk for faith , hope and charity does little to offend , but if saccharine earnestness were a crime , the film 's producers would be in the clink for life .\n",
      "\n",
      "the enormous comic potential of an oafish idiot impersonating an aristocrat remains sadly unrealized .\n",
      "\n",
      "there are as many misses as hits , but ultimately , it finds humor in the foibles of human behavior , and it 's a welcome return to the roots of a genre that should depend on surprises .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 3\n",
      "\n",
      "this is such a dazzlingly self-assured directorial debut that it 's hard to know what to praise first .\n",
      "\n",
      "haynes has so fanatically fetishized every bizarre old-movie idiosyncrasy with such monastic devotion you 're not sure if you should applaud or look into having him committed .\n",
      "\n",
      "tadpole is a sophisticated , funny and good-natured treat , slight but a pleasure .\n",
      "\n",
      "#########################\n",
      "some sentences with predicted label 4\n",
      "\n",
      "well worth the time .\n",
      "\n",
      "once you get into its rhythm ... the movie becomes a heady experience .\n",
      "\n",
      "the movie has lots of dancing and fabulous music .\n",
      "\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "#Predicting the labels the Test Set\n",
    "predictions = np.argmax(model.predict(x_test),axis=1)\n",
    "\n",
    "print(\"Plotting some predictions on the Test Set\")\n",
    "print(\"#########################\")\n",
    "\n",
    "for label in range(5):\n",
    "    print(\"some sentences with predicted label \"+str(label)+\"\\n\")\n",
    "    subset = np.argwhere(predictions==label)\n",
    "    if len(subset)>0:\n",
    "        selection = np.random.choice(subset.flatten(),3)\n",
    "        for i in selection:\n",
    "            print(sentences[\"test.X\"][i])\n",
    "    print(\"#########################\")\n",
    "    \n",
    "np.savetxt('logreg_lstm_y_test_sst.txt', predictions, fmt='%i', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "#One could maybe try a Multi-Class SVM on the outputs of the LSTM Layer\n",
    "#However, I did not find time to investigate more on the topic\n",
    "#with the several projects for January....\n",
    "#But this TP was very interesting, thanks !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
